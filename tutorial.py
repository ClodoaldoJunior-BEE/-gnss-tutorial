# -*- coding: utf-8 -*-
"""Tutorial

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nb_jZDpULoYKcnwW-AQerDaVd2MFDeSy

# Tutorial para aplicação de linguagem de programação *Python* para utilização de dados dos sinais GNSS e construção de modelos capazes de prever perdas de sincronismo associadas a Ionosférica

Iniciação Científica 

Bolsista: Clodoaldo de Souza Faria Júnior, Curso: Bacharelado em Engenharia Elétrica

Orientadores: Alexandre Ataíde Carniato e Bruno César Vani, Docentes no Instituto Federal de Educação, Ciência e Tecnologia de São Paulo - Câmpus Presidente Epitácio

# Introdução 
A tecnologia GNSS é de extrema importância para determinação da posição de qualquer objeto na Terra ou próxima dela, desta forma, é extremamente relevante o controle da qualidade deste sinal, entretanto, quando o mesmo se propaga pela Ionosfera, este pode sofrer diversos efeitos.

Entre os efeitos, se destacam os causados pela Ionosfera, por conta da sua diferença entre densidade eletrônica, desta forma, influenciando sinais eletromagnéticos ao se propagarem por esta, podendo acarretar na degradação do sinal e em perdas de sincronismo no receptor, o qual é o objeto de estudo do presente trabalho.
![texto alternativo](https://openclipart.org/image/800px/194758)

# Motivações e objetivos do tutorial

Ocorre uma preocupação com os efeitos causados pela cintilação ionosférica no Brasil, pelo fato de que, se tem uma pré-disposição para a ocorrência do deste fenômeno em influencia da densidade de elétrons na região equatorial, conhecida como Anomalia de *Appleton* ou AIE (Anomalia de Ionização Equatorial), consistindo em duas faixas de alta densidade de plasma ionosférico localizados nas regiões tropicais que circulam paralelamente ao equador magnético.

Ocorrendo a maior concentração de elétrons na faixa de 20º ao Sul e 20º ao Norte, principalmente no início da tarde, observando-se que o Brasil é um dos países mais afetados pelas irregularidades e cintilações ionosféricas.

Dito isto, o presente tutorial, visa a apresentação das principais técnicas de *Data Science* que podem ser empregadas para a análise de dados dos satélites GNSS. O mesmo apresentará todos os conceitos fundamentais necessários para o entendimento das técnicas e metodologias propostas, sempre progredindo da forma mais natural possível, partindo desde a utilização de poucas variáveis, até um estudo mais complexo e robusto.

Toda via, no presente tutorial, visa-se propor técnicas como Redes Neurais Artificiais (RNAs ou *Artificial Neural Networks* - *AANs* ou *NNS*), Árvores de Decisão (ADs ou *Decision Trees* - *DTs*), Regressões Lineares, Não-lineares e Logísticas, ainda como ferramentas que podem ser utilizadas em conjunto com as abordadas anteriormente, como a Análise de Componentes Principais (ACP ou * Principal Component Analysis* - *PCA*)sendo que todas estas serão trabalhadas com ênfase em obter no final algoritmos capazes de prever perdas de sincronismo associadas a ocorrência de cintilação ionosférica dos sinais GNSS com ênfase na frequência L1.

Sendo ainda realizado um estudo acerca do comportamento de inúmeras variáveis e índices ionosféricos propostas na literatura, entretanto, ainda serão expostas algumas que não são comumente citadas e utilizadas na literatura, entretanto, se mostraram eficiente em evidenciar momentos que precedem uma perda de sincronismo. Desta forma os objetivos deste tutorial são:

Objetivos gerais: Fornecer todo conhecimento necessário desde princípios básicos para empregar a linguagem de programação *Python* afim de prever perdas de sincronismo e trabalhar com dados dos sinais GNSS com uma taxa de amostragem de 50 Hz.

Objetivos específicos: 
1.	Apresentar e justificar os benefícios da utilização do *Google Colab* juntamente com a linguagem de programação *Python*;
2.	Apresentar e descrever todos os conceitos e passos necessários para a aplicação dos principais algoritmos de inteligência artificial;
3.	Identificar e apresentar propostas para resolução dos principais problemas apresentados em algoritmos de inteligência artificial;
4.	Apresentar e descrever as funções e ferramentas importantes disponibilizadas pelas bibliotecas que o *Python* apresenta, assim como identificar benefícios e malefícios de emprega-las;
5.	Apresentar e conceituar os principais métodos de avaliação de modelos, assim como utiliza-los para identificar pontos de melhoria ou descarte de um determinado modelo obtido;
6.	Apresentar ao leitor possíveis modelos que podem ser empregados em cada uma das áreas que se façam necessárias ou pelo menos norteá-lo do modelo que recorrerá, estendendo-se um pouco além da aplicação em sinais GNSS;
7.	Promover interesse por estudo nas áreas de algoritmos inteligentes, quanto na área de sinais GNSS e mitigação ionosférica.

# Metodologia adotada

#Software utilizado
Outro ponto importante é que todos os códigos foram construídos no software MATLAB que consiste em um ambiente de desktop para análise interativa e processsos de designer com uma linguagem de progação que expressa matemática e matriz diretamente, e conta também com toolboxes que são desenvolvidas profissionalmente, rigorosamente testadas e totamente documentas, entretanto este é um software pago e muito caro.

Desta forma, este tutorial visa a adpatação de todos os códigos que foram contruídos inicialmente no MATLAB na linguagem Python que é uma linguagem de progamação em alto nível, interpretada, de script, imperativa, orientada a objetos, funcional, de tipagem dinâmica e forte e atualmente possui um modelo de desenvolvimento comunitário, aberto e gerenciado pela organização sem fins lucrativos Python Software Foundation.

#Porque ulilizar o Google Colab?

Alémd e ser fácild e ser utilizado, o mesmo é bastante flexível em sua configuração e faz muito do trabalho pesado para o progamador, ainda contando com:

> Suporte a Python 2.7 e 3.6;

> Acelerador GPU grátis;

> Bibliotecas pré-instaladas: Todas as principais bibliotecas Python, como o TensorFlow, o Scikit-learn, o Matplotlib, entre muitas outras, estão pré-instaladas e prontas para serem importadas;

> Construído com base no Jupyter Notebook;

> Recurso de colaboração (funciona com uma equipe igual ao Google Docs): o Google Colab permite que os desenvolvedores usem e compartilhem o Jupyter notebook entre si sem precisar baixar, instalar ou executar qualquer coisa que não seja um navegador;

> Suporta comandos bash;

> Os notebooks do Google Colab são armazenados no drive.

#Bibliotecas extremamente úteis para estudos em Data Science

Para trabalhar funções matemáticas complexas e diferentes matrizes, a biblioteca NumPy é a mais utilizada, sendo através desta é possível trabalhar com vetores, arrays e matrizes grandes e multidimensionais. Sendo que na prática, o que esta biblioteca consegue fazer é trabalhar com funções matemáticas e estáticas com poucos comandos.

Outra biblioteca útil é a Pandas, sendo que esta é uma estrutura de dados de alto nível que fornece ferramentas de análises, toda via, os métodos desta servem para combinar, filtrar e agrupar dados, sendo ideal para trabalhar com séries temporais e permitindo a organização de dados em colunas em linhas, ou seja, esta pode funcionar como uma versão sofisticada de uma planilha tradicional. Ainda através do método to excel() é possível ler planilhas do Excel e ainda gerar arquivos xlsx.

Com a Scikit-learn é possível fazer dimensionamento, classificação, regressão, seleção de modelo, redução e Clustering, além desta ter base em outras duas bibliotecas, sendo estas a NumPy e SciPy.

Matplotlib, através desta biblioteca é possível a criação de gráficos 2D, diagramas e plotagens, entretanto, como se trata de uma biblioteca de baixo nível é necessário a adição na hora de gerar os gráficos, entretanto, trata-se de uma biblioteca bem flexível. Toda via, caso seja necessário pode-se utilizar a biblioteca Seaborn (devida da Matplotlib), em que os padrões que esta possui são mais sofisticados e incluem mapas de calor e gráficos de violino.

A Ploty é uma biblioteca que possibilita a visualização iterativa e refinada, sendo inclusos ainda alguns recursos ternários, de contorno e 3D, outro ponto é que a curva de aprendizagem desta é bem que pequena, com isto, com alguns tutoriais já se consegue desenvolver gráficos complexos, sendo o único desafio entender a documentação oficial.

# Tratamento dos dados 
Inicialmente a primeira preocupação é com o tipo de técnicas que podem ser utilizadas, sendo que como se trata de uma aplicação real, é de extrema importância a utilização de um método com aprendizado supervisionado, ou seja, para cada conjunto de dados proposto, tem-se uma saída esperada. Neste caso, pode-se ter por exemplo:

> x1n (Índice A), x2n (Índice B) e yn (Saída esperada)

Para cada n tem-se dois valores de entrada (x1 e x2) e uma saída esperada (y), sendo que neste caso, a saída esperada poderia adotar dois valores, como:

> y = 1, caso o momento precede uma perda de sincronismo e y = 0, caso o momento analizado em questão não precede uma perda de sincronismo

Dito isto, como os dados utilizados em questão tem uma taxa de amostragem de 50Hz, ou seja, medições a cada 0.02 segundos, quando esta medição não houve, significa que houve uma perda, com isto, para achar os momentos que precedem uma perda de sincronismo basta aplicar a dupla diferença na coluna TOW (Time of Week) dos dados brutos e quando a dupla diferença for maior que aproximadamente 0.02 aquela janela de tempo analisada recebe como saída esperada o valor 1, caso contrário recebe o valor de saída esperada como 0. 

Com isto inicialmente é necessário acessar os arquivos disponíveis no Google Drive que neste exemplo, correspondem a uma hora de observações do satélite GPS 27 em uma taxa de amostragem de 50 Hz no ano de 2013.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Após isso pode-se utilizar a biblioteca Pandas para a realização da leitura do dado tipo xlsx, sendo que esta é uma biblioteca de software criada para a linguagem Python para a manipulação e análise de dados, sendo que em particular, oferece estruturas e operações para manipular tabelas numéricas e séries temporais, sendo que o comando para instalar ela caso seja necessário é:

> !pip install pandas

Enquanto a NumPy é uma biblioteca que é usada principalmente para fazer cálculos em arrays multidimensionais, sendo que esta fornece um grande conjunto de funções e operações de biblioteca que ajudam os programadores a executar facilmente cálculos numéricos.

Vale destacam também, que na mesma podem ser utilizados modelos de Machine Learning, realizar processos de processamentos de imagem e computação gráfica e tarefas matemáticas (se igualando muito ao software MALTAB em alguns casos), sendo que o comando para instalar esta biblioteca caso seja necessário é:

> !pip install numpy
"""

import pandas as pd
import numpy as geek
data = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/Pasta1.xlsx')
data.head()
tow = data['TOW']
diff_tow = geek.diff(tow)
losses = (diff_tow > 0.025)

"""Desta forma, ao analisar a variável "losses" observa-se que os momentos que contém o valor True são aqueles que precedem uma perda de sincronismo e aqueles com o valor igual a False são os que não precedem uma perda de sincronismo. Sendo importante ressaltar que a proporção de momentos que precedem uma perda de sincronismo em relação a momentos que não precedem uma perda de sincronismo é quase irrelevante.

No banco de dados utilizado como exemplo, é possível observar isso, sendo que como os dados são amostrados em 50Hz, em uma hora, teoricamente ter-se-ia 180 mil conjuntos de dados, entretanto tem-se 179860 conjuntos, ou seja, apenas aproximadamente 0.077% dos dados estão faltando.

Com isto, um dos cuidados principais que deve-se tomar ao construir um modelo para a presente aplicação é a quantidade de dados que se tem em cada um dos conjuntos de dados, ou seja, tentar igualar a quantidade de conjuntos que precedem uma perda de sincronismo e que não precedem uma perda de sincronismo, pelo simples fato de que, se tomarmos o exemplo acima, a acurácia do modelo seria de 99.92%, ou seja, um modelo extremamente assertivo em teoria, entretanto, este não seria capaz de fazer nenhuma ou quase nenhuma generalização a respeito de momentos que precedem uma perda de sincronismo.

# Visualização dos dados e os principais índices de cintilação
Após identificado os momentos que precedem as perdas de sincronismo, se faz possível a visualizar e a realização de algumas analises iniciais a respeito do comportamento dos dados, sendo de extrema importância a comparação entre o comportamento dos dados nos momentos que precedem uma perda e nos momentos que não precedem uma perda, possuindo como finalidade buscar padrões que ocorrem quando dado conjunto precede uma perda de sincronismo.

O principal índice utilizado para descrever as cintilações é baseado no comportamento da intensidade do sinal, sendo que estes valores podem ser obtidos através da soma dos quadrados das componentes da quadratura do sinal disponível nos dados 50Hz, ou seja:

> Intensidade = Icorr^2 + Qcorr^2

E com isto, pode-se obter o índice S4 para todo o conjunto de dados através da seguinte forma, sendo que o índice S4 pode ser calculado para qualquer intervalo de tempo superior ou igual a dez segundos, pelo fato de que, quando calculado para intervalos menores que dez segundos este apresenta muitas instabilidades, não sendo um índice confiável.

Sendo que aqui pode ser utilizado para plotagem dos gráficos a biblioteca matplotlib, que através desta é possível a criação de gráficos em 2D de forma simples e com poucos comando, sendo que a API foi projetada para ser compatível com o MATLAB que apesar de ser referência na área de processamento numérico, não é opensource. Dessa forma, caso seja necessário, pode-se instalar a biblioteca pelo comando:

> !pip install matplotlib
"""

import numpy as np
import matplotlib.pyplot as plt
import math 

ww = 500
icorr = data['Icorr']
qcorr = data['Qcorr']

intensity = []
intensity_db = []
intensity = icorr**2 + qcorr**2

aux = []
for i in range(ww):
  intensity_db.append(10*(math.log10(intensity[i]))/np.mean(intensity))

intensity

plt.figure(figsize=(10,10))
s4 = np.std(intensity[1:ww])/np.mean(intensity[1:ww])

plt.subplot(1,2,1)
plt.subplots_adjust(hspace=0.5, wspace=0.3)
plt.plot(data['TOW'][1:ww],intensity[1:ww],'*')
plt.title('Comportamento da intensidade ao longo de dez segundos \n Índice S4:'+str(round(s4,4)))
plt.xlabel('TOW (Time of Week)')
plt.ylabel('Intensidade')

plt.subplot(1,2,2)
plt.plot(data['TOW'][1:ww],intensity_db[1:ww],'.')
plt.title('Comportamento da intensidade em dB ao longo de dez segundos \n Índice S4:'+str(round(s4,4)))
plt.xlabel('TOW (Time of Week)')
plt.ylabel('Intensidade [dB]')

"""Com isto tem-se o Índice S4 para um intervalo de tempo de 10 segundos, desta forma, faz-se a mesma operação para todo o conjunto de dados de intensidade, sendo que neste caso, todos os índices e variáveis utilizadas foram calculadas em intervalos de 10 segundos.

Sendo para o conjunto de dados são calculados vários índices como:

1.   Média dos ângulos formados entre a quadratura do sinal (Icorr e Qcorr)
2.   Índice S4
3. Índice Sigma-phi
4. Índice SI
5. Os menores valores absolutos da fase filtrada pelo filtro Butterworth de 6º ordem do tipo passa alta, com faixa de rejeição de 0.1 Hz
6. Valores do fading mais intenso observado na janela de tempo analisada
7. Parâmetro alpha da distribuição alpha-mu ajustado a amplitude e índice S4 observado na janela de tempo analisada
8. Parâmetro mu da distribuição alpha-mu ajustado a amplitude e índice S4 observado na janela de tempo analisada
9. Valor do erro quadrático médio do resíduo da fase da onda portadora, sendo que este é calculado a partir da diferença entre um polinômio interpolador de grau 3 e a fase real
10. Quantidade de vezes que a intensidade passou de -6dB
11. Quantidade de vezes que a intensidade passou de -9dB
12. Quantidade de vezes que a intensidade passou de -12dB
13. Quantidade de vezes que a intensidade passou de -15dB
14. Flutuação pico-a-pico estimada de uma relação empírica a partir do índice S4

Vale ressaltar que o principal impedimento para a realização destes cálculos é os valores que faltam, desta forma, um cuidado a ser tomado é completar os momentos de dados faltantes com valores não numéricos, para que quando for analisado determinado intervalo de tempo, tenha-se certeza que os dados requeridos pertencem a aquele intervalo de tempo.

Sendo que aqui pode-se utilizar a biblioteca math, sendo que através destes é possível a realização de diversas operações aritméticas importantes, como arredondamento, média, fatorial, seno, cosseno, tangente, utilização de números complexos, entre tantas outras funções.
"""

import numpy as np
import math

ww = 500
tow = data['TOW']
loss = []
s4 = []
sigma_phi = []
si = []

inicial = tow[1]
losses = (np.diff(tow) > 0.025)
time_inicial = np.arange(inicial, inicial+3000, 10)
time_final = np.arange(inicial+10, inicial+3000+ww, 10)

k = 1
for i in range(np.size(losses)):
  if losses[i] == True:
    loss.append(tow[i])

output = []
for i in range(np.size(time_inicial)):
  for j in range(np.size(loss)):
    if loss[j] >= time_inicial[i] and loss[j] <= time_final[i]:
      output.append(1) 
    else:
      output.append(0)

"""Desta forma, através da utilização do código acima é possível identificar os momentos de interesse do estudo e rotular os conjuntos de dados, como já foi dito anteriormente, desta forma, a seguir calcula-se várias variáveis de interesse e depois analisa-se cada uma delas através de inúmeras técnicas de visualização de dados, possuindo como finalidade utilizar somente as mais eficazes em evidenciar momentos que precedem uma perda de sincronismo.

Aqui outra biblioteca famosa pode ser utilizada, a Scikit-learn ou sklearn, sendo que esta é uma das mais famosas bibliotecas de modelos com aprendizado de máquina, como Máquinas de Vetores de Suporte, Regressão, Agrupamento, K-means, Redes Neurais Artificiais (RNAs), como será também utilizada posteriormente, entretanto, aqui esta será utilizada para a realização de operações aritméticas e tratamento de dados. Vale ressaltar que caso seja necessário, esta pode ser instalada pelo comando:

> !pip install sklearn
"""

s4 = []
si = []
sigma_phi = []
std_diff = []
pfluc = []

import numpy as geek
import matplotlib.pyplot as plt
from sklearn import preprocessing

aux = []
#len(time_inicial)
for i in range(4):
  for j in range(len(data['TOW'])):
    if data['TOW'][j] >= time_inicial[i] and data['TOW'][j] <= time_final[i]:
      aux.append(j)

  fase = []
  intensidade = []
  amplitude = []
  intensidade_db = []
  fase_diff = []
  std_diff = []
  pfluc = []

  for k in range(len(aux)):
        fase.append(data['Fase'][k])
        intensidade.append((data['Qcorr'][k]**2 + data['Icorr'][k]**2))
        intensidade_db.append(10*(math.log10(intensidade[k])))
        amplitude.append(math.sqrt(intensidade[k]))
        fase_diff = geek.diff(geek.diff((fase)))

  s4.append(np.std(intensidade)/np.mean(intensidade))
  si.append((max(intensidade)-min(intensidade))/(max(intensidade)+min(intensidade)))
  sigma_phi.append(np.std(fase)/np.mean(fase))
  std_diff.append(np.std(std_diff))
  pfluc.append(27.5*(s4[i]**(1.26)))



"""Calculado algumas das variáveis pode-se observar o comportamento destas ao longo do tempo, sendo que uma importante variável que pode ser analisada para isso é a tripla diferença da fase ao longo de um intervalo de tempo, por exemplo."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
fase_1 =[]
fase_1.append((fase_diff - fase_diff.min())/(fase_diff.max()-fase_diff.min()))
plt.plot(data['TOW'][0:2000],fase_diff[0:2000],'.')
plt.title('Comportamento da tripla diferença da fase em função do tempo')
plt.xlabel('TOW (Time of Week)')
plt.ylabel('Tripla diferença da fase normalizada')

"""Umas das mais importantes variáveis que podem ser analisadas para que possivelmente possam ser utilizadas para a construção de modelos capazes de realizar a predição de momentos que precedem uma perda de sincronismo, são: os parâmetros alpha e mu da distribuição alpha-mu, sendo que estes são estimados a partir da amplitude do sinal e o índice S4.

Sendo que a função de densidade de probabilidade que é mais utilizada nestes casos é a Nakagami-M que consegue descrever bem a variabilidade da amplitude dos sinais cintilantes, entranto, recentemente mostrou-se que o fenômeno de cintilação de amplitude ionosférica pode ser melhor modelado usando o modelo alpha-mu, sendo que este explora a não linearidade do meio de propagação, associando os fenômenos de desbotamento físico aos paramêtros alpha-mu.

Com tudo em Moraes et al. (2012) empiricamente parametrizou-se esta distribuição em função da gravidade da cintilação em si, resultando em uma distribuição mais flexível que é especialmente adpatada aos eventos de cintilação ionosféricas e que melhor se encaixa nos dados experimentais.

A flexibilidade do modelo alpha-mu proporciona uma melhor capacidade de ajuste aos dados de cintilação, sendo que vale mencionar a relação inversa entre os parâmetros alpha e mu, pois alpha aumenta e mu diminiu e vice-versa. Na Figura a baixo tem-se a ilustração das variações de aplha e mu para diferentes valores de S4.![texto alternativo](https://agupubs.onlinelibrary.wiley.com/cms/asset/499b1596-a0b8-4f13-a084-29ef9bd4eba3/rds20093-fig-0001-m.jpg)
"""

from sympy import *
import numpy as np
import scipy.special
import pandas as pd

def f(x,B1,B2): 
  a=B1-((scipy.special.gamma(x(2)+(1/x(1)))^2)/(scipy.special.gamma(x(2))*scipy.special.gamma(x(2)+(2/x(1)))-scipy.special.gamma(x(2)+(1/x(1)))^2));
  b=B2-((scipy.special.gamma(x(2)+(2/x(1)))^2)/(scipy.special.gamma(x(2))*scipy.special.gamma(x(2)+(4/x(1)))-scipy.special.gamma(x(2)+(2/x(1)))^2));
  y = [a,b]
  return y

def FadingsCoeff(intensity,s4):
  intensity = intensity.values.tolist()
  r = (np.sqrt(intensity[i]))
  B1 = np.mean(r)**2/(np.mean(r**2)-np.mean(r)**2)
  B2 = 1/s4**2
  x = solve(f, B1, B2)
  al=x(1);
  m=x(2);
  return a1, m

FadingsCoeff(pd.DataFrame(intensity[1:3000]),1)

intensity = pd.DataFrame(intensity[1:3000]).values.tolist()

"""A fase do sinal geralmente é filtrada com um filtro Butterworth de 6º ordem com a finalidade de remoção do efeito da geometria orbital do satélite e do erro do relógio do receptor ao longo do tempo, sendo que a frequência de corte é de 0.1 Hz, esta fase sem aplicação de filtragem na literatura é chamada de detrended phase.

Sendo que para a aplicação da filtragem primeiramente é necessário transformar a fase de ciclos para radianos, após isto pode-se realizar a transformação, sendo que após fica a cargo da utilização deixar a fase filtrada em radianos ou em ciclos.

Sendo que para a realização da filtragem será utiliza a biblioteca SciPy, nesta contém várias ferramentas dedicadas a problemas comuns em computação científica, podendo ser utilizada para interpolação, integração, otimização, processamento de imagens, estatísticas, funções especiais, etc.
"""

from scipy import signal
import matplotlib.pyplot as plt
import math

fase = data['Fase']
b, a = signal.butter(6, (0.1/(50/2)), 'highpass', analog=True)
w, h = signal.freqs(b, a)

fase_radianos = []
for i in range(len(fase)):
  fase_radianos.append(fase[i] * (2 * math.pi))

fase_filtrada = signal.lfilter(b,a,fase_radianos)

fase_ciclos = []
for i in range(len(fase)):
  fase_ciclos.append(fase_radianos[i] / (2 * math.pi))

plt.subplot(1,2,1)
plt.semilogx(w, 20 * np.log10(abs(h)))
plt.title('Resposta do filtro Butterworth de 6º ordem')
plt.xlabel('Frequência [rad/s]')
plt.ylabel('Amplitude [dB]')
plt.margins(0, 0.1)
plt.grid(which='both', axis='both')
plt.axvline(100, color='green') # cutoff frequency
plt.show()

plt.subplot(1,2,1)
tempo = data['TOW'][0:3000]
fase_1 = fase_filtrada[0:3000]
plt.plot(tempo,fase_1,'.')
plt.xlabel('TOW (Time of Week)')
plt.ylabel('Fase filtrada em ciclos')

"""Outra importante variável que pode ser calculada é o resíduo da fase, sendo que este pode ser calculado a partir do erro quadrático médio da diferença entre a fase real e a fase estimada a partir de um polinômio interpolar, o qual, será realizado uns estudo acerca do melhor grau para utilização."""

from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

fase = data['Fase']
tempo = data['TOW'][0:3000]
fase_1 = fase[0:3000]
ideal_1 = interp1d(tempo,fase_1)
ideal_3 = interp1d(tempo,fase_1, kind='cubic')

fase_ideal_1 = np.linspace(0,3000,num = 3000)
fase_ideal_3 = np.linspace(0,3000,num = 3000)

residuo_1 = fase[0:3000] - fase_ideal_1
residuo_3 = fase[0:3000] - fase_ideal_3

plt.plot(tempo,residuo_1,'.')
plt.plot(tempo,residuo_3,'*')
plt.xlabel('TOW (Time of Week)')
plt.ylabel('Valores do resíduo da fase')
plt.title('Resíduo da fase ao longo do tempo')
plt.legend('best')

type(fase_ideal_1)

"""#Sistemas inteligentes (SIAD)

Desde a revolução industrial o homem busca pela substituição do trablho manual por máquinas, sendo que nos meados do século XX surgiu o termo IA (Inteligencia Artificial), sendo que esta é capaz de tomar uma decisão certa dado um conjunto de informções e uma variedade de ações possíveis.

![texto alternativo](https://openclipart.org/image/800px/282974)

A principal diferença dos sistemas inteligentes para os sistemas tradicionais é a capacidade de manipular símbolos que representam o mundo real, sendo capazes de trabalhar efizamente com o conhecimento, possuindo como principais caractériscticas:

> Habilidades para usar conhecimento para desempenhar tarefas

> Resolver problemas

> Capacidade de trabalhar com problemas complexos com aplicaçoes reias

> Podem ser desenvolvindos com o uso de algumas técnicas, tais como: Aquisição de Conhecimento, Aprendizado de Máquina, Redes Neurais, Agentes e MultiAgentes,Lógica Fuzzy, Mineração de Textos, Redes Bayesianas,  Computação Evolutiva,  Mineração de Dados, Sistemas Especialistas

> Raciocínio baseado em casos, e entre outros.

# Redes Neurais Artificais (RNAs) e seus conceitos fundamentais
Uma das principais técnicas aplicadas em SIAD é a Rede Neural Artifical que consiste em uma estrutura complexa interligada por elementos de processamento simples (neurônios), que possuem a capacidade de realizar operações de cálculos em paralelo, para o processamento de dados e representação de conhecimento.

Sendo que o primeiro modelo foi introduzido em 1943, entretanto só ganhou notorieade quando introduziu-se os algoritmos com treinamento com o backpropagtion, que permite a realizção de um treinamento porterior para aperfeiçoar o modelo, sendo que este é o algoritmo que será utilizado no presente estudo.

O algoritmo que será utilizado em questão é o MLP que consiste no algoritmo Perceptron estendido, sendo assim, este é compreendido como um algoritmo de aprendizado supervisionado que é capaz de construir uma função a partir de conjuntos de dados e suas devidas saídas esperadas, com a finalidade de que após a construção do modelo, este seja capaz de prever se um conjunto de dados em um certo intervalo de tempo precede uma perda de sincronismo ou não.
![texto alternativo](https://openclipart.org/image/800px/293227)

O passo a passo da construção do modelo MLP consiste em:
1.   Atribuir valores aleatórios para os pesos e limiares
2.   Calcular os valores dos neurônios na camada oculta e de saída
3.  Calcular os erros dos neurônios da camada de saída e oculta com a finalidade de correção dos pesos, desta forma, posteriormente atualizando os pesos dos neurônios das camadas de saída e oculta
4. Repete-se o processo até que o passo 2 satisfaça um critério de erro

Sendo que para isto, os dados que serão utilizados para testes devem serem capazes de englobar o comportamento do sinal GNSS ao longo do ano, já que este varia de acordo com a época do ano e inúmeras outras coisas, desta forma, para treinamento serão utilizados dados de Janeiro a Junho de 2013, sendo que destes, serão selecionados a mesma quantidade de conjuntos de uma hora de dados que tiveram algum momento que precedeu uma perda de sincronismo e a quantidade de dados de uma hora que não tiveram nenhum momento que precedeu uma perda de sincronismo.

Desta forma, com esta seleção, evita-se que o modelo possa sofrer de overfitting, ou seja, um ajuste tão grande ao conjunto de dados de treinamento que este se torna inviável para utilização de verdade, ou seja, quando qualquer conjunto de dado que não foi utilizado para treinamento é apresentado para a RNA, esta não é capaz de realizar a sua previsão, ficando desta forma, classificado pela "sorte".

Outro destaque se dá pela utilização de validação cruzada, sendo esta uma técnica para avaliar a capacidade de generalização de um modelo, a mesma é amplamente empregada em problemas onde o objetivo da modelagem é predição.

O conceito da validação cruzada é particionar em subconjuntos mutuamente exclusivos, e posteriormente, o uso de alguns destes subconjuntos para estimação dos parâmetros do modelo (dados de treinamento), sendo os subconjuntos restantes (dados de validação ou teste) empregados na validação do modelo.

# Construção de uma RNA com base em banco de dados utilizado de perdas de sincronismo em sinais GNSS 
Sendo que para esta parte será utilizado um banco de dados que foi calculado previamente, de todas as variáveis, dividindo em momentos que precedem uma perda de sincronismo e momentos que não precedem uma perda de sincronismo, com isto, como já foi dito anteriormente para conjuntos de dados que correspondem a momento que precedem uma perda tem-se a saída esperada com valor igual a 1 e quando não, tem-se a saída esperada com valor igual a 0. 

Vale ressaltar que o motivo para utilização do banco de dados com as todas as variáveis calculadas é por conta do tempo que leva para calcula-las, sendo que mesmo com um computador potente o processo foi extremamente lento, toda via, o banco de dados conta com todos os cálculos de todas as variáveis que já foram citadas anteriormente para os casos mais significativos entres os meses de Janeiro e Julho do ano de 2013.

Inicialmente, serão utilizadas apenas duas variáveis, para que faça possível um maior entendimento do que está acontecendo através da plotagem de gráficos, e posteriormente, será utilizado todas as varáveis, sendo que esta não possui uma visualização gráfico, por contar com 14 dimensões.
"""

import pandas as pd
from math import ceil
from sklearn.neural_network import MLPClassifier

sjcu_loss = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - COM PERDAS.xlsx')
sjcu_perfect = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - SEM PERDAS.xlsx')

sjcu_loss.head()
sjcu_perfect.head()

tamanho = len(sjcu_loss)
x = pd.concat([sjcu_loss[{'S4','SI'}][0:tamanho], sjcu_perfect[{'S4','SI'}][0:tamanho]]) 

y = []
for i in range(tamanho):
  y.append(1)

for i in range(tamanho):
  y.append(0)

len(x)

"""A biblioteca Sklearn tem uma função que pode ser utilizada para separar os dados em dois conjuntos distintos, sendo um para treinamento e outro para teste, ou seja, para avaliação do quão bom está o modelo.

> sklearn.model_selection
"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,shuffle=True)

len(x_test)

"""Utilizando a biblioteca é possível a construção do modelo que possibilitará a classificação, sendo que este terá os seguintes parâmetros:

> Método de solução: tipo "adam", este refere-se a um otimizador baseado em gradiente estocástico proposto por Kingma, Diederik e Jimmy Ba, funcionando muito bem em conjuntos de dados relativamente grandes e termos de tempo de treinamento e validação, sendo que para conjuntos de dados pequenos, recomenda-se a utilização do método "lbfgs", o que leva a uma convergência mais rápida e melhor desempenho.

> Alpha: 1e-5 (também conhecido como parâmetro de penalidade L2 ou termo de regularização).

> Quantidade de neurônios na camada oculta: 1024 e 128, respectivamente.

> Máximo de iterações: 1500.

> Também se declara random_state = 1 para a determinação de números aleatórios para inicialização dos pesos e bias.
"""

mlp_model = MLPClassifier(solver='adam',alpha=1e-5,hidden_layer_sizes=(100),random_state=1,max_iter=1500)
print(mlp_model)

mlp_model.fit(x_train,y_train)
pred_mlp = mlp_model.predict(x_test)

y

"""# Matriz de confusão
A matriz de confusão é uma tabela que mostra a frequência de classificação para cada classe do modelo, sendo que esta ilustra as frequências:
1.   Verdadeiro positivo (true positive - TP), este ocorre quando a classe que é buscada é prevista pelo modelo corretamente, neste caso quando o modelo prediz o valor 1 e a saída esperada é 1
2.   Falso positivo (false positive - FP), ocorre quando a classe que é buscada é prevista pelo modelo de forma incorreta
3.   Falso verdadeiro (true negative - TN), refere-se a classe que não era buscava e é prevista corretamente pelo modelo, neste caso quando a saída esperada era igual a 0 e o modelo predisse o valor 0
4.   Falso negativo (false negative - FN), ocorre quando a classe que não era buscada foi prevista incorretamente

# Métricas de avaliação de modelos
Durante o processo de criação de um modelo é necessário a medição da qualidade de suas predições, sendo que, as mais usuais são:

1.   Acurácia (Taxa de acerto), esta é a métrica mais simples, sendo basicamente o número de acertos (positivos) dividido pelo número total de amostras, entretanto, esta métrica neste caso não se torna tão importante pelo fato de que, mesmo que seja ignorada a classe que tem o valor de saída esperada igual a 1, ou seja, a mesma não tem sensibilidade quando o conjuntos de dados tem cluster desproporcionais, por este motivo que no começo do código foi pego a mesma quantidade de conjuntos para compor cada um dos clusters, entretanto, caso isto não tivesse sido feito, haveria necessidade de tomar muito cuidado com a utiização desta métrica
2.   F1 Score, esta é uma média harmônica entre precisão e recall, sendo ela é ótima para ser utilizada em datasets que possuem classes desproporcionais e o seu modelo não emite probabilidades, como é o caso do exemplo atual
3. Precisão (precision), consiste no número de exemplos classificados como pertencentes a uma classe que são realmente daquela classe (positivos verdadeiros), dividido pela soma entre este número e o número de exemplos classificados nesta classe, mas que pertencem a outra classe (falsos positivos)
4. Recall, refere-se ao número de conjuntos classificados como pertencentes a uma classe, que realmente são daquela classe, dividido pela quantidade total de exemplos que pertencem a esta classe, mesmo que sejam classificados em outra
5. AUC - Area Under the ROC Curve, esta é a métrica mais interessante para analisar classes desproporcionais, sendo que nela mede-se a área sob uma curva formada pelo gráfico entre a taxa de exemplos positivos, que realmente são positivos, e a taxa de falsos positivos, sendo que como vantagem em relação ao F1 Score, é medido o desempenho do modelo em vários pontos de corte, não necessariamente atribuindo exemplos com probabilidade maior que 50% para a classe positiva, e menor, para a classe negativa

![texto alternativo](https://openclipart.org/image/800px/288474)
"""

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score
from sklearn import metrics

print('Acurácia: ',metrics.accuracy_score(pred_mlp,y_test)*100,'%')
print('F1 Score: ',metrics.f1_score(pred_mlp,y_test)*100,'%')
print('Precisão: ',metrics.precision_score(pred_mlp,y_test)*100,'%')
print('Recall: ',metrics.recall_score(pred_mlp,y_test)*100,'%')
#print('ROC AUC Score: ', metrics.roc_auc_score(pred_mlp, y_test, average='macro',multi_class='ovr')*100,'%')

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(20, 12))
plt.plot([0,1],[0,1],'r--')

probs = mlp_model.predict_proba(x_test)
# Probabilidade de leitura da segunda classe
probs = probs[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, probs)
roc_auc = auc(fpr,tpr)

label = 'MLP Classifier AUC:' + ' {0:.2f}'.format(roc_auc)
plt.plot(fpr,tpr,c='g',label=label,linewidth=4)
plt.xlabel('Taxa de falso positivo',fontsize=16)
plt.ylabel('Taxa de verdadeiro positivo',fontsize=16)
plt.title('Características operacionais do receptor (ROC)',fontsize=16)
plt.legend(loc = 'lower right',fontsize=16)

"""A curva CAP representa o gráfico entre os resultados positivos cumulativos no eixo Y e os resultados cumulativos no eixo X."""

total = len(y_test)
class_1_count =len(sjcu_loss)
class_0_count = len(sjcu_perfect)
plt.figure(figsize=(20, 12))

probs = mlp_model.predict_proba(x_test)
probs = probs[:,1]
model_y = [y for _, y in sorted(zip(probs, y_test),reverse=True)]
y_values = np.append([0],np.cumsum(model_y))
x_values = np.arange(0, total + 1)
plt.plot(x_values,y_values,c = 'b',label='MPL Classifier',linewidth=4)

plt.xlabel('Total de observçaões',fontsize=16)
plt.ylabel('Observações da classe 1',fontsize=16)
plt.title('Perfil de precisão cumulativa (CAP)',fontsize=16)
plt.legend(loc = 'lower right',fontsize=16)

"""Para realizar a análise do plot realizado acima, é necessário:

1. Desenhar uma linha vertical em 50% do eixo x em que o modelo construído cruza

2. No ponto em ele corta o modelo de previsão, desenhar uma linha horizontal para cortar o eixo y

3. Calcular a porcentagem da classe 1 identificada e a contagem total de rótulos da classe 1

Obtido a porcentagem, se analisa ela da seguinte forma:

> Menos de 60%: Modelo não pode ser utilizado

> 60% até 70%: Modelo ruim

> 70% até 80%: Modelo bom

> 80% até 90%: Modelo muito bom

> Mais de 90%: Bom de demais para ser verdade, sendo que neste caso é sempre preciso verificar se não ocorreu overfitting, tornando o modelo inutilizável
"""

plt.figure(figsize=(20, 12))
plt.plot(x_values,y_values,c='b',label='MPL Classifier',linewidth=4)

# Ponto onde a linha vertical cortará o modelo treinado
index = int((50*total/100))

## Linha vertical de 50% a partir do eixo x
plt.plot([index, index],[0,y_values[index]],c='g',linestyle='--')

## Linha horizontal para o eixo y do modelo de previsão
plt.plot([0, index],[y_values[index],y_values[index]],c='g',linestyle='--')

class_1_observed = y_values[index] * 100 / max(y_values)

plt.xlabel('Total de observçaões',fontsize=16)
plt.ylabel('Observações da classe 1', fontsize=16)
plt.title('Perfil de precisão cumulativa (CAP)',fontsize=16)
plt.legend(loc = 'lower right', fontsize=16)
print("Porcentagem: {0:.2f}%".format(class_1_observed))

"""#Problemas de *Overfitting* e *Underfitting*

Quando um modelo é construído baseado em *Machine Learning* (Aprendizado de Máquina) podem ser utilizados basicamente três tipos de aprendizados como já foi citado anteriormente, sendo estes:

> Aprendizado por reforço, este é o método em que a máquina tenta aprender qual pe a melhor ação a ser tomada, dependendo das circuntâncias essa ação será executada

> Aprendizado supervisionado, que consiste em um conjunto de técncia para ajustar os parâmetros de função para que satisfaçam algumas condições que são dadas pelos valores das saídas esperadas

> Aprendizado não supervisionado, netse caso o modelo tenta descobrir padrões sem saídas esperadas, de uma forma geral, este tipo acha uma representação própia dos dados aprensentados a ele, condensando a informaçõa em pontos mais relevantes e também mais simples, entretanto, este muitas vezes deve ser utilizado com cautela, pois o mesmo não tem uma "base" para seguir, de uma forma mais informal, "este modelo faz a classificação e obtenção de informações da forma que conseguir, sendo ela certa ou não"

Entre estas três abordagens a melhor delas depende da aplicação, sendo que cada problema possui suas pecularidades, e uma maneira de resolução que funcionou bem para um problema Alpha pode naõ funcionar para um problema Beta, com isto, acima de tudo, a máquina é um instrumento de conveniência, ou seja, o desempenho da inteligência artifical está inteiramente ligado à inteligência humana de quem a treinou.

Sendo que fatores como o dilema Bias-Variance exisgem um cuidado do pesquisador para que a máquina forneça de fato os resultados requeridos, entretanto, técnicas de regularização e validação cruzada se destacam em "guiar" a máquina e garantir um meio termp entre acurácia na amostra e poder de generalização. COm isto, os dois principais problemas podem ser o overfitting e o underfitting que consistem em:

> *Overfitting* ocorre quando o modleo se adptou tão bem aos dados que este não consegue generalizar novos dados, ou seja, o modelo acaba que "decorando" os dados de treuno, consequentemente não aprendendo de fato a diferenciar aqueles dados quando precisar enfrentar novos testes

> *Underfitting* ocorre quando o modelo não se adapta bem sequer aos dados com que este foi treinando, sendo dispensável

#Dowload dos dados
Caso seja necessário baixar dados é possível entrar na plataforma ISMR Query Tool e baixa-los, entretanto, quando é necessário a utilização de muitos dados, isto se torna inviável, desta forma, necessita-se fazer um código para baixa-los de forma automatica, sendo que um dos cuidados que deve-se tomar é de realizar o download dos dados somente com ângulo da máscara de elevação superior aos valores proposto por Mendonça (2013), evitando assim possibilitando a utilização dos dados de forma otimizada, possibilitando maior abrangência das estações de rastreio, sem que haja interferências de outras fontes, senão, atmosféricas.

> MANA/MAN2: 6º/42º

> PALM: 25º

> PRU2: 25º

> MACA/MAC2: 15º/16º

> SJCU: 22º

> POAL: 26º

#Árvores de decisão e seus conceitos fundamentais
As Árvores de Decisão são modelos estatíscos que utilizam o treinamento supervisionado e uma estratégia chamada "dividir para conquistar" (divide-and-conquester) para a aprendizagen dis padrões de um determinado conjunto de dados, sendo que com esta ferramenta o problema proposto é composto em sub-problemas mais simles e recursivamente está técnica é aplicada a cada subu-problema.

Entre os principais benefícios de sua aplicação tem-se:

> Facilidade de interpretação e apresentação para terceiros

> Possibilidade de testes de caixa branca, o que implica, que seu funcionamento interno pode ser observado e ver os passos que a árvore está tomando ao ser modelada

O processo de contrução de uma árvore de decisão recebe o nome de induction process (processo de indução) e este demanda um alto custo computacional, apesar de que a árvore de decisão é conhecida pela sua velocidade, isto ocorre por conta de que a construção desta envolve uma análise detalhada dos dados, que depedem da dimensão e do tamanho destes, tornando a indução trabalhosa, sendo que uma possível solução para isto é a utilização desta juntamente com a Análise de Componentes Principais ou Transformada Discreta de Karhunen-Loeve para a redução da dimensinalidade dos dados

# Construção de um modelo baseado em Árvores de Decisão utilizando banco de dados de perdas de sincronismo em sinais GNSS
"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier,export_graphviz
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np

"""Carregando a base de dados e igualando a quantidade de amostras de cada um dos clusters, ou seja, a categoria de momentos que precedem uma perda de sincronismo possuirá o mesmo tamanho do que a categoria de momentos que não precedem uma perda de sincronismo."""

sjcu_loss = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - COM PERDAS.xlsx')
sjcu_perfect = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - SEM PERDAS.xlsx')

sjcu_loss.head()
sjcu_perfect.head()

tamanho = len(sjcu_loss)
x = pd.concat([sjcu_loss[{'S4','SI'}][0:tamanho], sjcu_perfect[{'S4','SI'}][0:tamanho]]) 

y = []
for i in range(tamanho):
  y.append(1)

for i in range(tamanho):
  y.append(0)

"""Dividindo os dados em treinamento e teste."""

X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3)

"""Verificando o tamanho dos conjuntos de entrada que foram divididos para teste e treinamento."""

X_train.shape, X_test.shape

"""Verificando o tamanho dos conjuntos de saída que foram dividos para saída  de teste e treinamento."""

y_train.shape, y_test.shape

"""As AD classificam instancias através de uma estrututa de árvore começando da raiz até uma folha, sendo que estas são construídas e representadas através de dois elementos, os nós e arcos que conectam os nós.

Sendo que para ocorrer a tomada de decisão, o fluxo começa no nó raiz até chegar a um nó folha e, em seguida, toma um decisão. Sendo que cada nó da árvore deonta um teste de um atributo, e os ramos denotam os possíveis valores que o atributo pode tomar.

Desta forma, toda AD começa com um nó raiz e termina em uma folha, sendo importante ressaltar que as árvores de decisão não convergem em nenhum ponto, elas segmentam o seu caminho a medida que que os nós são processados.

Criação do objeto classificador usando a função:

> DecisionTreeClassifier()

Entre os métodos que podem ser utilizados apra avaliação da utilização de cada atributo nas regras de decisão da árvore se destaca a avaliação do ganho de informação e entropia. Sendo que neste mede-se o ganho de informação antes e depois da seleção de uma atributo, sendo que busca-se o nó que melhor preve o resultado da classifcação.

Com isgto, o ganho de informação mde a redução da entropia causada pela partição do conjunto, desta forma, para medir esta redução compara-se o grau de entropia do nó-pai (antes da divisão) com o grau de entropia do nó-filho (depois da divisão) e o atributo que gerar a maior diferença é escolhido como condição teste.
"""

clf = DecisionTreeClassifier()

"""Quantos aos parâmetros que precisam ser definidos no classificador DecisionTrreeClassifier, são:

> criterio: medida de divisão, senod que aqui definimos se utilizaremos metrícas como Indíce Gini, Chi-Square, Information Gain ou a redução da variância;

> splitter: estratégia utilizada para devidir o nó de decisão;

> min_samples_split: corresponde ao número de amostras mínimas para considerar um nó ára divisão;

> min_samples_leaf: corresponde ao número de amostras mínimas no nível folha;

> max_depth: corresponde a proximidade máxima da árvore.

Com isto, treina-se o modelo com os conjuntos separados para treinamento, neste caso chamados de X_train e Y_train.
"""

clf = clf.fit(X_train,y_train)

"""Para verificação das fectures mais importantes para o modelo treinado utiliza-se o comando abaixo, sneod que este retorna um array com o valor de importância cada variável.

> clf.feature_importances_
"""

clf.feature_importances_

"""Sempre que para a realização de uma amostragem de forma mais organizada pode-se utilziar o seguinte código:"""

for feature,importancia in zip(sjcu_loss.columns,clf.feature_importances_):
    print("{}:{}".format(feature, clf.feature_importances_))

"""Para testar o modelo utiliza-se o objeto classificador nos X_test e após para verificaçaõ acerca da qualidade do modelo, compara-se os dados previstos com o modelo e os dados da saída esperada, que estão apresentado neste caso, na variável Y_test."""

predict = clf.predict(X_test)
predict

from sklearn import metrics
print(metrics.classification_report(y_test,predict))

"""# Plotagem dos algoritmos baseados em Árvores de Decisão

Por útlimo para representação da forma gráfica pode ser utilizar o código abaixo, sendo que antes disto, é necessário instalar três bibliotecas para utilziação, da seguinte forma:

> !pip install ipywidgets

> !pip3 install graphviz

> !pip3 install pydot

E a seguir, utiliza-se o código abaixo para a representação gráfica da árvore, entretanto, como não treinamos o modelo sem especificar nenhum parâmetros, ela grasceu de forma indefinida e ficou grande demais, com sito, a visualização da mesma fica impossibilitada.
"""

!pip install ipywidgets
!pip3 install graphviz
!pip3 install pydot
import pydot
import graphviz

dot_data = export_graphviz(clf,out_file=None,feature_names=x.columns,class_names=['0','1'],filled=True,rounded=True,proportion=True,node_ids=True,rotate=False,label='all',special_characters=True)    
graph = graphviz.Source(dot_data) 
#graph = graphviz.Graph(format='png')
graph

"""#Cuidados que deve-se tomar ao construir o modelo baseado em Árvore de Decisão
Um dos principais problemas com as Árvores de Decisões é que esta pode criar modelos excessivamente complexos, dependendo do conjunto de treinamento apresentado a esta, sendo que o algoritmo estende a sua profundidade ao ponto de classificar perfeitamente os conjuntos de dados, desta forma, para evitar o overfitting que já foi mencionado anteriormente pode-se adotar duas metodologias:

> Pré-poda que consiste em estabelecer um critério de parada do algoritmo de criação da árvore, podendo ser estabelecido para transformar um nó corrente em um nó folha, utilizando o ganho de informação, quando todas as divisões possíveis usando um atributo A geram ganhos menor que um valor pré-estabelecido.

> Enquanto a Pós-poda consiste em substituir uma sub-árvore por um nó folha que representa a classe com mais frequência no ramo, sendo que esta calcula a taxa de erro causada por esta mudança, pois se a taxa de erro for menor que um valor pré-estabelecido, a árvore então é podada e caso contrário, não ocorrerá poda.

Sendo que com a especificação de alguns parâmetros se faz possível a visualização do comportamento da árvore.

Caso seja necessário pode-se instalar as bibliotecas necessarias pelos seguintes comandos:
"""

from ipywidgets import interactive
from IPython.display import SVG,display
from graphviz import Source

"""Renderizando a árvore de forma interativa"""

# feature matrix
X,y = x, y

# feature labels
features_label = y

# class label
class_label = ['0','1']

def plot_tree(crit, split, depth, min_samples_split, min_samples_leaf=0.2):
    estimator = DecisionTreeClassifier(random_state = 0,min_samples_leaf=min_samples_leaf,min_samples_split=min_samples_split,max_depth = depth,criterion = crit,splitter = split)
    estimator.fit(X, y)
    graph = Source(export_graphviz(estimator,out_file=None,feature_names=features_label,class_names=class_label,impurity=True,filled = True))
    display(SVG(graph.pipe(format='svg')))
    return estimator

inter=interactive(plot_tree,crit=["gini","entropy"],split=["best","random"],depth=[1,2,3,4,5,10,20,30],min_samples_split=(1,5),min_samples_leaf=(1,5)) 
display(inter)

"""É possivél também a extração das regras do modelo baseado em árvore de decisão a partir de uma determinada amostra de dados, a partir do seguinte código:"""

# Commented out IPython magic to ensure Python compatibility.
def extract_rules(code):
    sample_id = code
    node_indicator = estimator.decision_path(X)

    leave_id = estimator.apply(X)
    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:node_indicator.indptr[sample_id + 1]]

    print('\nFeatures usadas para predizer a amostra %s' % (sample_id))

    for f,v in zip(df_diabetes.columns,X.iloc[sample_id].values):
        print('%s = %s'%(f,v))

    print('\n')      
    for node_id in node_index:
        if leave_id[sample_id] == node_id:
            continue

        if (X.iloc[sample_id, feature[node_id]] <= threshold[node_id]):
            threshold_sign = "<="
        else:
            threshold_sign = ">"

        print("id do nó de decisão %s : (atributo %s com valor = %s %s %s)"
#               % (node_id,
                 df_diabetes.columns[feature[node_id]],X.iloc[sample_id, feature[node_id]],threshold_sign,threshold[node_id]))
                 
    pred =estimator.predict(X.iloc[sample_id].values.reshape(1, -1))
    print(pred) 
    print("\tClasse => %s" %pred)

"""# *Random Forest* (Floresta Randômica)

A RF (*Randon Forest*) é um algoritmo de aprendizagem supervisionada em que são criadas combinações (*ensembles*) de Árvores de Decisão (AD), já apresentada anteriormente. Sendo que a ideia principal desta é a criação de várias Árvores de Decisão e combina estas para obter uma predição com maior acurácia e mais estável.

Sendo que diferente da AD normal que procura pela melhor característica ao fazer a partições do nós, a RF busca a melhor característica em um subconjunto aleatório das características, com isto, este processo cria uma grande diversidade, o que geralmente implica na geração de modelos melhores.

Outra importante característica das RF é a facilidade para se medir a importância relativa de cada característica (*feature*) para a predição. Sendo que na biblioteca *Sklearn* que é utilizada, tem-se uma excelente ferramenta já implementada que mede a importância das características analisando quantos nós das árvores usam uma dada característica, desta forma, reduzem impureza geral da RF.

Através da inspeção da importância das característica pode-se decidir quais features ficarão fora do modelo, já que estas, por sua vez não tem relevância para o processo de predição, sendo que, em geral quanto maior o número de features, mais provável que o modelo sofra do overfitting, que já foi discutido anteriormente.

#Parâmetros importantes para a utilização da *Random Forest* da biblioteca Sklearn

> *n_estimators*: este parâmetro indica o número de árvores que serão construídas pelo algoritmo antes de fazer uma média das predições, sendo que, em geral, uma quantidade elevada de árvores aumenta a performance e torna as predições mais estáveis, entretanto, torna a computação mais lenta;

> *max_features*: indica o número máximo de características que serão utilizadas pela RF para a construção de uma dada árvore;

> *min_sample_leaf*: indica o número mínimo de folhas que devem existir em uma dada árvore;

> *n_jobs*: indica quantos processadores o algoritmo pode utilizar, sendo que se este tiver o valor 1, o mesmo só poderá usar um processador e se tiver o valor -1 significa que não há limite para o mesmo;

> *random_state*: este parâmetro também conhecido como semente, faz com que o modelo seja replicável, sendo que se o modelo tiver o mesmo valor para random_state resultará no mesmo modelo, logicamente, contando com os mesmos dados de treinamento;

> *cob_score*: também chamado de oob sampling é um método de validação cruzada para a RF, sendo que neste tipo de sampling (amostragem), cerca de um terço dos dados não é utilizado no treinamento e pode ser utilizado para avaliar a performance, sendo que estas amostras são chamadas de out of bag samples. Outra técnica similar a esta é o método de validação cruzada leave one out, mas sem nenhum custo computacional extra.

#Vantagens e desvantagens da *Random Forest*

Uma das principais vantagens da RF é o fato desta poder ser utilizada tanto para regressão, quanto para classificação, sendo fácil de visualizar a importância relativa que ela atribui para cada features (característica) nas suas entradas.

A RF é também considerada um algoritmo fácil e acessível, pois seus hiperparâmetros com valores default geralmente produzem um bom modelo de predição, toda via, a quantidade de hiperparâmetros não é tão grande e são muito fáceis de serem compreendidos.

Um problema que era facilmente encontrado na Árvore de Decisão convencional e na RF se lida muito bem é o overfitting, sendo que na maior parte do tempo o mesmo não ocorrerá facilmente, isto porque se já árvore suficiente na floresta, o classificador não irá sobreajustar o modelo.

Sendo que a maior limitação da RF é que a grande quantidade de árvores pode tornar o algoritmo lento e ineficiente para predições em tempo real, sendo que em geral, estes algoritmos são rápidos para treinar, mas muito lentos para fazer predições depois de treinados.

Por conta de que uma predição com mais acurácia requer mais árvores, o que faz o modelo ficar mais lento, sendo que pode haver situações onde a performance em tempo de execução e outras abordagens são mais apropriadas. E por último, mas não menos importante, a RF da mesma forma que a AD são ferramentas de modelagem preditiva e não descritiva, sendo que para descrição dos relacionamentos nos seus dados, deve-se adotar outra abordagem.

# Criação e plotagem da *Random Forest*

Caso a biblioteca não esteja instalada, pode-se utilizar o comando abaixo:
"""

!pip install sklearn.tree

"""Para a criação do modelo baseado em RandomForest utiliza-se a mesma biblioteca utilizada até então, sendo esta a SkLearn, toda via, o módulo que será utilizado é o RandomForestClassifier para a criação do modelo em si e o módulo tree para realizar a plotagem da Árvore de Decisão obtida.

Sendo que para a criação da mesma, utiliza-se os mesmo passos iniciais que foram já realizados nos outros modelos, divindindo os conjuntos de dados em dados de treinamento e teste do modelo. Toda via, neste passo não será repetido o código, pois o mesmo já foi apresentado inúmeras vezes no decorrer deste tutorial.

Vale ainda resaltar que a figura que será plotada é em formato PNG, com fundo transparente, desta forma, facilitando a utilização desta em possivéis estudos. Sendo possível salvar a mesma e copiá-la de forma simples.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn import tree

rf = RandomForestClassifier(n_estimators=100,random_state=0)
rf.fit(X_train, y_train)
rf.estimators_

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)
tree.plot_tree(rf.estimators_[0],filled = True); ##,feature_names = fn, class_names=cn,
fig.savefig('rf_individualtree.png')

"""O código abaixo plota esimações de Árvores de Decisões Randômicas, no formato PNG, sendo que como foi falado anteriormente, este fato permite manusear a imagem plotada de forma facilitada. 

Toda via, para a plotagem da mesma, ainda se utiliza o módulo tree da SkLearn, entretanto, diferentemente do código anterior, neste se faz uma plotagem iterativa de alguns dos modleos obtidos.
"""

fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (10,2), dpi=3000)
for index in range(0, 5):
    tree.plot_tree(rf.estimators_[index],filled = True, ax = axes[index]);
    ##,feature_names = fn, class_names=cn
    axes[index].set_title('Estimator: ' + str(index+1), fontsize = 11)
fig.savefig('rf_5trees.png')

"""Por último, caso desejar-se obter cada um dos parâmetros que foram utilizados para a criação dos modelos obtidos para RandomForest, pode-se utilizar o .*estimators_*.

Ao chamar esta função obtém-se todoso os parâmetros importantes de forma organizada, sendo necessário apenas o conhecimento do que são os parâmetros fornecidos por esta e caso não se conheça estes, pode-se consultar ainda a documentação disponivél do site da própia biblioteca.

Toda via, vale resaltar o fato de que o Python tenta ficar o mais próximo possível da língua inglesa, com isto, caso se realizada a tradução de cada termo e comparados com os parâmetros conhecidos, talvez se faça possível o entendimento dos mesmo, sendo que o entendimento desta forma fica apenas a cargo do entedimento do leitor sobre a língua inglesa e a respeito dos parâmetros do algoritmo.
"""

rf.estimators_

"""#Análise de Componentes Principais e seus conceitos fundamentais

A Análise de Componentes Principais ou Principal Component Analysis (PCA) é uma técnica que promove a redução de dimensionalidade, entretanto perdendo quantidade insignificantes de características, sendo que em modelos esta se faz de grande importância pelo fato de que com uma grande quantidade de características (features)  muito correlacionadas entre si, os modelos podem sofrer facilmente de sobreajuste;

A PCA calcula uma projeção dos dados em algum vetor que maximize a variação dos dados e perca a menor quantidade possível de informação, estes vetores são os autovetores da matriz de correlação das características de um conjunto de dados.

Para a aplicação da PCA, segue-se os seguintes passos:

  1. Calcula-se a matriz de correlação das colunas de característica (features) e encontra os autovetores desta matriz;
  2. Calcula-se a projeção de todas as características nos autovetores encontrados no passo 1.

#Aplicação da Análise de Componentes Principais utilizando banco de dados de perdas de sincronismo em sinais GNSS

Para a aplicação da PCA é necessário padronizar os dados, um vez que esta analise é influenciada com base na escala dos dados, sendo um prática comum normalizar os dados antes de alimentá-los a qualquer algoritmo de aprendizagem de máquina.

Para aplicação da normalizçaão, pode-se importar um módulo da biblioteca Sklearn e utilizando funções como:

> StandardScaler;

> breast_dataset;

> fit_transform;
"""

sjcu_loss = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - COM PERDAS.xlsx')
sjcu_perfect = pd.read_excel('/content/drive/My Drive/ARQUIVOS IC/SJCU - SEM PERDAS.xlsx')

sjcu_loss.head()
sjcu_perfect.head()

tamanho1 = len(sjcu_loss) #------ caso precisar deixar com o mesmo número de perdas e de não perdas
x = pd.concat([sjcu_loss[:][0:len(sjcu_loss)], sjcu_perfect[:][0:len(sjcu_perfect)]]) 

#caso precisar das saídas esperadas
y = []
for i in range(tamanho):
  y.append(1)

for i in range(tamanho):
  y.append(0)

from sklearn.preprocessing import StandardScaler
x = StandardScaler().fit_transform(x)

"""Sendo que a aplicação da StandardScaler, cada recurso dos dados será normalmente distruibuido de forma que irá dimensionar a distribuição para uma média de zero e um desvio padrão de um.
Verifica-se também se os dados normalziados têm uma média zero e desvio padrão de um.
"""

np.mean(x), np.std(x)

"""Importanto o módulo PCA da biblioteca *Skelearn* e passando o valor *n_componentes = n*, finalmente tem-se as principais *n* principais componetes dos dadados."""

from sklearn.decomposition import PCA

pca_breast = PCA(n_components=4)
principalComponents_x = pca_breast.fit_transform(x)

principal_breast_Df = pd.DataFrame(data = principalComponents_x,columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])

"""Com isto, a variavél *principalComponents_x* possuirá os valores de quatro principais componentes para todas a amostras de *x*, desta forma, pode-se visualiza-los, pelo código apresentado abaixo, ou da forma que o leitor achar conveniente para a aplicação necessária."""

principal_breast_Df.tail()

"""Um vez obtida as componentes principais, pode-se encontrar a quantidade de informação ou variância que cada componente principal possui depois de projetar os dados em um subespaço dimensional inferior.

Ou seja, utilizando a análise PCA, transforma-se as *N* váriaveis ortogonais em *N* novas váriaveis, onde as *W* primeiras variáveis apresentão quase que em totalidade a variação dos dados originais, com isto, pode-se desprezar *N-W* dimensões dos dados, promovendo assim uma redução de dimensionalidade.
"""

print('Explained variation per principal component: {}'.format(pca_breast.explained_variance_ratio_))

"""Sendo que para visualização serão pegas apenas as duas primeiras principais componentes, para que se possa ter uma melhor visão de como suas amostras são distribuídas entre as duas classes."""

plt.figure()
plt.figure(figsize=(10,10))
plt.xticks(fontsize=12)
plt.yticks(fontsize=14)
plt.xlabel('Principal Component - 1',fontsize=20)
plt.ylabel('Principal Component - 2',fontsize=20)
plt.title("Principal Component Analysis of Breast Cancer Dataset",fontsize=20)
targets = ['Benign', 'Malignant']
colors = ['r', 'g']
for target, color in zip(targets,colors):
    indicesToKeep = breast_dataset['label'] == y
    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'principal component 1'], principal_breast_Df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)

plt.legend(targets,prop={'size': 15})

"""Apesar da técnica PCA se mostra extremamente atraente deve-se tomar cuidado ao utilizá-la, sendo que se forem utilizadas menos dimensões que o necessário tem-se perdas de informações.

Enquanto a utilização de dimensões a mais, podem inserir ruídos, o que pode acarretar em uma interpretação errônea dos dados. Com isto, uma das práticas mais adotadas para a escolha do número de Componentes Principais que serão utilizadas é o método do gráfico *Scree*, sendo que neste é apresentado uma curva que liga os autovalores em um gráfico de escala logística ou normal.

Através deste verifica-se o ponto após a sua primeira inflexão, adotando este como o número adequado de dimensões a serem utilizadas.

#Entre todos os algoritmos, qual deles escolher?

Comumente na área de *Data Science* tem-se a dúvida de qual modelo utilizar, por conta da imensa diversidade que se tem dos mesmo, desta forma, a primeira necessidade para realizar esta escolha deve ser com respeito ao tipo de aprendizado que será utilizado, sendo que isto varia muito de acordo com a necessidade e forma com que está disposta os dados que serão utilizados na construção do modelo.

Quanto os tipos de aprendizado, estes já foram detalhados no tutorial, mas aqui serão apresentados novamente, entretanto, dando ênfase nos principais modelos que são utilizados e que comumente nas mais gerais aplicações apresentam ótimos desempenhos, quando construídos com o devido cuidado.

1. **Aprendizagem supervisionada** - Modelos com este tipo de aprendizado são extremamente recomendados para casos que serão apegados na prática e necessita-se de um alto controle do que o modelo pode predizer, ou seja, necessita-se de uma alta acurácia do modelo, como por exemplo, pode-se citar a situação que o modelo será empregado para que um carro autônomo funcione adequadamente, desta forma, o mesmo não pode ficar falhando como bem entender, por conta de que a cada falha geraria gasto.

  Apesar do exemplo citado ser um pouco extremista, na grande esmagadora dos modelos que são construídos para aplicação real, tem-se a utilização de modelos com aprendizado supervisionado.

  Entretanto, uma das principais desvantagens do aprendizado supervisionado é exatamente com o mesmo funciona, por conta de ser necessárias saídas conhecidas para o conjunto de dados que serão apresentados ao modelo.
  
  Toda via, os modelos com aprendizado supervisionado têm como principal tarefa encontrar parâmetros ótimos que ajustem um modelo que possa prever rótulos desconhecidas em outros conjuntos de dados, que na *Data Science*, recebem o nome de conjuntos de testes.

  No caso de os rótulos serem números reais, a tarefa recebe o nome de regressão, com isto, o modelo é chamado de **Modelo de Regressão com Aprendizado Supervisionado**, entretanto, se o rótulo vem de um conjunto finito e não ordenado, a tarefa chama-se classificação, com isto o modelo recebe o nome de **Modelo de Classificação por Aprendizado Supervisionado**.


2. **Aprendizagem não-supervisionada** - Os modelos que empregam este tipo de aprendizagem conseguem trabalhar com menos informações sobre o conjunto de treinamento, diferentemente do que foi apresentado anteriormente, na aprendizagem não-supervisionada não se tem a necessidade do rótulo nos conjuntos de treinamento.

  Com isto, os modelos que são construídos com aprendizagem não-supervisionada conseguem faz extrações de características dos conjuntos de dados que muitas vezes não visíveis de outra forma, pelo fato de que estes fazem o seu treinamento a partir do entendimento de como os conjuntos de dados de treinamento se comportam, toda via, conjuntos de dados que diferem largamente de todos os outros *clusters* (grupos), são considerados anomalias (*outliers*).


3. **Aprendizagem por reforço** - Diferentemente dos tipos de aprendizado que foram apresentados anteriormente, que se baseiam basicamente em como o conjunto de treinamento é apresentado, na aprendizagem por reforço tem-se um sistema de recompensa cumulativa.

  Ou seja, este processo de aprendizagem investiga como agente de *software* devem agir em determinados ambientes, possuindo como finalidade maximizar alguma noção de recompensa cumulativa, sendo que para exemplo, tem-se algo parecido com o que na biologia recebe o nome de adaptação ao ambiente natural.


  Desta forma, são destacados os modelos comumente utilizados, aonde se deve o seu emprego, principais características, funcionalidades e tipo de aprendizado:

1. **Árvores de Decisão** - Os modelos baseados em Árvores de Decisão possuem aprendizado de forma supervisionada e tem como principais características a facilidade de interpretação do modelo resultante por um terceiro, sendo que este pode não entender nada sobre *Data Science* e mesmo assim compreender totalmente o que a representação gráfica ou em forma escrita está querendo "dizer".

  Entretanto, um dos maiores problemas das Árvores de Decisão é que estas podem ficar bastante complexas, podendo facilmente ocorrer o *overfitting*, sendo que tal fenômeno ocorre quando o modelo obtido se ajustou excessivamente aos conjuntos de dados de treinamento, ao ponto que, o mesmo não tem capacidade de generalização, ou seja, a resposta predita para dados desconhecidos para este modelo com *overfitting* não difere muito de respostas aleatórias. Desta forma, para situações mais complexas e robustas, é recomendada a utilização de modelos baseados em *Random Forests*

  Dentro dos algoritmos de Árvores de decisões ainda existem vários algoritmos que utilizam desta metodologia, sendo que não se pode determinar qual o melhor em formas gerais, mas, dependendo do problema, um algoritmo pode ser mais eficiente que outros. Dentre estes algoritmos, se destacam *ID3*, *C5*, *C4.5*, *Assistant*, *Classification and Regression Trees*.

2. **Redes Neurais Artificiais** - As Redes Neurais Artificiais (RNAs) são modelos baseados no funcionamento dos neurônios biológicos, sendo que estas podem ser utilizadas de inúmeras formas e existem RNAs para todos os tipos de necessidades.

  Estas são extremamente simples e intuitivas, na grande maioria dos algoritmos existentes, uma vez que entende-se os modelos lineares, são flexíveis, o que as torna ideais para resolver os mais diversos tipos de problemas e por último, mas não menos importante, são absurdamente efetivas quanto a qualidade dos seus resultados.

  Quantos as desvantagens da RNAs têm-se que os modelos obtidos através desta são comumente gigantescos, consumindo muita energia e recursos computacionais, outro ponto é que treinar uma RNA é extremamente difícil, dado o formato não convexo da função de custo.

  Além das duas desvantagens citadas anteriormente, as RNAs assim como as Árvores de Decisão, estão sujeitas a *overfitting* com facilidade, devido a sua alta capacidade, por conta disso, em termos práticos, quando tem-se dados não tão abundantes (< 10000), os resultados não costumam ser melhores do que os obtidos com quaisquer outros algoritmos empregados na *Data Science*.

  Entre os inúmeros algoritmos de Redes Neurais Artificiais, destacam-se as Redes Neurais *Multilayer Perceptron*, que funcionam com aprendizado supervisionado e se destacam e inúmeras aplicações em que se treinam em um conjunto de pares entrada-saída e aprendem a modelar a correlação entre entradas e saídas, as Redes Convolucionais que podem ser utilizadas para classificação e identificação de aspectos de dados visuais, as Redes Neurais Recorrentes que são especialmente úteis para processamento de dados sequenciais, como som, dados de séries temporais ou linguagem natural e as LSTM (*Long Short-Term Memory*) que consistem em uma variação da Rede Recorrente que podem ser utilizadas para o problema de *vanishing gradient*, possuindo muitas aplicações práticas, incluindo geração automática de texto, análise de séries temporais e processamento de linguagem natural. 


3. **K-médias** - Este é um algoritmo de agrupamento iterativo que classifica objetos num determinado pré-definido *N* de *clusters* (grupos).

  A principais vantagens são que todos os conjuntos de dados separados para treinamento são automaticamente atribuídos a um *cluster* e a localização inicial do centroide do grupo pode variar, ou seja, esta permite estabelecer condições iniciais de dependência, toda via, o algoritmo K-médias é bem simples e de fácil compreensão, entretanto, pode atender perfeitamente como base de comparação em uma gama de problemas.

  Entretanto, antes do algoritmo ser iniciado, deve ser escolhido o número *N* de *clusters* que serão considerados e ainda, todos os conjuntos de dados são forçados a pertencer a um grupo.

Desta forma, evidencia-se que cada algoritmo tem seu destaque em uma determinada área, sendo que apresentou-se apenas os principais, valendo ressaltar que existem algoritmos desenvolvidos para utilização em áreas especificas, com isto, cabe ao projetista testar e avaliar qual modelo melhor se adapta a determinada utilização.

Entretanto, como opinião pessoal do autor, evidencia-se a utilização das Redes Neurais Artificias, pelo fato de que estas podem ser desde algoritmos simples, até os mais complexos, sendo que praticamente para cada aplicação que se fizer necessária, vai existir uma RNA que se aplica de forma ótima no problema.

Com um destaque as Redes Neurais da Família *ART*, que conseguem lidar com a grande maioria dos problemas que as RNAs apresentam, como o dilema da plasticidade/elasticidade, ou seja, esta consegue aprender novos conjuntos de dados sem perdes o conhecimento previamente obtido e ainda estas RNAs tem versões para grande parte das aplicações necessárias.

Vale ressaltar ainda que a Rede Neural Artificial é o algoritmo mais estudado, dentre todos os citados, com isto está em constante evolução, ou seja, "a cada dia" surgem novos algoritmos de RNAs, cada vez mais robustos e complexos.

Toda via, ressalta-se a importância da utilização da Análise de Componentes Principais para redução da dimensionalidade dos dados, tornando assim os modelos mais otimizados do ponto de vista computacional.

Sendo que esta ainda remove recursos correlacionados, encontra correlações nos conjuntos de dados, reduz o excesso de condições, entretanto vale salientar que, as variáveis independentes torna-se menos interpretáveis, deve ser realizada uma padronização dos dados (no *Python* pode ser utilizado a função *StandardScaler* do *SkLearn*) antes de ser realizada a aplicação da ferramenta e embora seja pouca, ainda existe perda de informações.
"""